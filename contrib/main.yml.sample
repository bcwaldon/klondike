# s3_config_bucket is the S3 bucket that this cluster should use
# to store its cluster config data. This data contains cluster
# secrets, so access to this bucket must be restricted to those
# that NEED to use klondike to manage this cluster. This bucket
# does not need to be unique to this cluster, as klondike will
# use a namespace when placing objects in S3.
s3_config_bucket:

# aws_region is the AWS region in which the cluster will be deployed.
aws_region:

# aws_availability_zone is the AWS availability zone the cluster
# will be deployed in. This must be within the AWS region indicated
# by aws_region. For example, if aws_region were us-west-2, usable
# availablility zones include us-west-2a, us-west-2b and us-west-2c.
aws_availability_zone:

# coreos_image_id is the AMI ID used to deploy both controller
# and worker EC2 instances. This AMI must be available in the region
# indicated by aws_region.
#
# The "Beta" CoreOS release channel should be used when selecting a
# CoreOS AMI, as this provides the best trade-off of stability and
# up-to-date software. The current minimum version of CoreOS
# is 962.0.0, which is the first release with kubelet-wrapper.
#
# We provide a simple utility to retrieve this AMI ID via the command line:
#
#     $ contrib/coreos-image-id ${aws_region}
#
# Because there is no guarantee for the stability of the schema, it's not
# possible to guarantee stability of this utility. As a fallback, we also
# provide manual instructions:
#
# 1. Navigate to  https://coreos.com/os/docs/latest/booting-on-ec2.html
# 2. Select the "Beta Channel" tab in the image selector
# 3. Identify the image available in your region
# 4. Copy the AMI ID for the HVM AMI type
#
# Set `coreos_image_id` with the AMI ID retrieved via either method.
coreos_image_id:

# ubuntu_image_id is the AMI ID used to deploy bastion EC2 instances. We
# provide a simple utility to retrieve this ID via the command line:
#
#     $ contrib/ubuntu-image-id ${aws_region}
#
# Because there is no guarantee for the stability of the schema, it's not
# possible to guarantee stability of this utility. As a fallback, we also
# provide manual instructions:
#
# 1. Navigate to https://cloud-images.ubuntu.com/locator/ec2/
# 2. Search for "16.04 hvm:ebs-ssd"
# 3. Identify the AMI available in your region
# 4. Copy the AMI ID
#
# Set `ubuntu_image_id` with the AMI ID retrieve via either method.
ubuntu_image_id:

# aws_hosted_zone is a DNS zone delegated to AWS Route 53. This
# zone will be used to provide DNS resolution for the Kubernetes
# API and gateway. For example:
#aws_hosted_zone: "{{ cluster }}.kubernetes.example.com"
aws_hosted_zone:

# bastion_instance_type is the instance type used when deploying
# EC2 instances as bastions in your cluster. This instance is an
# administrative host, so it does not need a large amount of storage
# or compute resources.
#
# These instances must support the EBS-optimized boot flag.
# It is suggested that m4 or c4 instances are used, as these
# there is no additional cost to support EBS optimization.
bastion_instance_type: m4.large

# bastion_instance_root_disk_size_gb controls the size of the root
# disk provisioned for each bastion instance (in GB). The underlying
# disk is a gp2 EBS volume, not instance storage.
bastion_instance_root_disk_size_gb: 32

# aggregator_instance_type is the instance type used when deploying
# EC2 instances as log aggregators in your cluster.
#
# These instances must support the EBS-optimized boot flag.
# It is suggested that m4 or c4 instances are used, as these
# there is no additional cost to support EBS optimization.
aggregator_instance_type: m4.large

# aggregator_instance_root_disk_size_gb controls the size of the root
# disk provisioned for the aggregator instance (in GB). The underlying
# disk is a gp2 EBS volume, not instance storage.
aggregator_instance_root_disk_size_gb: 128

# gateway_instance_type is the instance type used when deploying
# EC2 instances for Kubernetes service ingress traffic.
#
# These instances must support the EBS-optimized boot flag.
# It is suggested that m4 or c4 instances are used, as these
# there is no additional cost to support EBS optimization.
gateway_instance_type: m4.large

# gateway_instance_count is the number of instances to deploy
# for the initial Kubernetes service ingress. This should scale
# with the amount of traffic routed into the cluster from
# external clients.
gateway_instance_count: 1

# gateway_instance_root_disk_size_gb controls the size of the root
# disk provisioned for each gateway instance (in GB). The underlying
# disk is a gp2 EBS volume, not instance storage.
gateway_instance_root_disk_size_gb: 32

# controller_instance_type is the instance type used when deploying
# EC2 instances for the Kubernetes control plane.
#
# These instances must support the EBS-optimized boot flag.
# It is suggested that m4 or c4 instances are used, as these
# there is no additional cost to support EBS optimization.
controller_instance_type: m4.large

# controller_instance_count is the number of instances to deploy
# for the initial Kubernetes control plane. 3 is recommended.
controller_instance_count: 3

# controller_instance_root_disk_size_gb controls the size of the root
# disk provisioned for each controller instance (in GB). The underlying
# disk is a gp2 EBS volume, not instance storage.
controller_instance_root_disk_size_gb: 32

# worker_instance_type is the instance type used when deploying
# EC2 instances for the Kubernetes worker pool.
#
# These instances must support the EBS-optimized boot flag.
# It is suggested that m4 or c4 instances are used, as these
# there is no additional cost to support EBS optimization.
worker_instance_type: c4.large

# worker_instance_count is the number of instances to deploy
# for the initial Kubernetes worker pool. Increasing this number
# increases the amount of resources available for pods.
worker_instance_count: 3

# worker_instance_root_disk_size_gb controls the size of the root
# disk provisioned for each worker instance (in GB). The underlying
# disk is an io1 EBS volume, not instance storage.
worker_instance_root_disk_size_gb: 64

# cluster_vpc_id is the ID of the VPC into which the Kubernetes
# cluster should be deployed. This VPC must already have an
# Internet Gateway attached. See the documentation for more
# information on how to create this network.
cluster_vpc_id:

# cluster_vpc_internet_gateway_id is the ID of the Internet
# Gateway attached to the cluster VPC.
cluster_vpc_internet_gateway_id:

# cluster_vpn_gateway_id is the ID of the VPN Gateway attached
# to the cluster VPC.
cluster_vpn_gateway_id:

# cluster_instance_network_cidr defines the range of IPs to use
# for the VPC, which contains all EC2 instances. These IPs must
# not conflict with the pod- or service-level networking.
cluster_instance_network_cidr: 10.0.0.0/23

# public_instance_subnet_cidr defines the range of IPs to use
# for internet-facing EC2 instances. This must be a subnet within
# the cluster_instance_network_cidr, not conflicting with any
# other *_instance_network_cidr values.
public_instance_subnet_cidr: 10.0.0.0/25

# controller_instance_subnet_cidr defines the range of IPs to use
# for controller EC2 instances. This must be a subnet within
# the cluster_instance_network_cidr, not conflicting with any
# other *_instance_network_cidr values.
controller_instance_subnet_cidr: 10.0.0.128/25

# worker_instance_subnet_cidr defines the range of IPs to use
# for worker EC2 instances. This must be a subnet within
# the cluster_instance_network_cidr, not conflicting with any
# other *_instance_network_cidr values.
worker_instance_subnet_cidr: 10.0.1.0/24

# cluster_pod_network_cidr defines the range of IPs to use for
# the kubernetes pod network. These IPs must not conflict with
# the instance- or service-level networking.
cluster_pod_network_cidr: 10.1.0.0/16

# cluster_service_network_cidr defines the range of IPs to use for
# the kubernetes service network. These IPs must not conflict
# with the instance- or pod-level networking.
cluster_service_network_cidr: 10.255.0.0/16

# cluster_service_kubernetes_ip is the service network IP that will
# automatically be assigned to the "kubernetes" service created by
# the cluster in the "default" namespace. This IP must be the first
# IP in the network defined by cluster_service_network_cidr.
cluster_service_kubernetes_ip: 10.255.0.1

# cluster_service_dns_ip is the service network IP address used by
# the DNS addon. This must be within the cluster_service_ip_range.
cluster_service_dns_ip: 10.255.0.10

# cluster_service_dns_zone is the DNS zone used by the DNS addon. For
# example, if "cluster.local" is used here, the default Kubernetes
# service will be available at "kubernetes.default.svc.cluster.local".
cluster_service_dns_zone: cluster.local

# peer_vpc_id is the ID of the VPC that will be routable to and from
# the Kubernetes cluster. A VPC peering connection will be created
# the peer VPC and the VPC created for Kubernetes.
peer_vpc_id:

# peer_network_cidr is the network of the VPC that will be routable to and
# from the Kubernetes cluster.
peer_network_cidr:

# peer_connection_id is the ID of the VPC Peering Connection already
# established between the cluster VPC and the peer VPC.
peer_connection_id:

# datadog_enabled controls whether or not the DataDog agent should be
# deployed across the cluster. If true, datadog_api_key must be set.
datadog_enabled: false

# datadog_api_key is the API key that the DataDog agent will use to
# push metrics about the Kubernetes cluster to the DataDog service.
# Only used if datadog_enabled is true.
#datadog_api_key:

# signalfx_enabled controls whether or not the SignalFX agent should be
# deployed to the cluster. If true, signalfx_api_key must be set.
signalfx_enabled: false

# signalfx_api_key is the API key that the SignalFX agent will use to
# push metrics about the Kubernetes cluster to the SignalFX service.
# Only used if signalfx_enabled is true.
#signalfx_api_key:

# logstash_enabled controls whether or not log data is collected and
# forwarded via logstash to an elasticsearch cluster. If true,
# logstash_elasticsearch_host must be set.
logstash_enabled: false

# logstash_elasticsearch_host is the <scheme>://<host> representing an
# elasticsearch cluster where logstash should send its logs. The
# scheme should typically be https. This field is only used if
# logstash_enabled is true.
#logstash_elasticsearch_host:
